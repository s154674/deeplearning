# -*- coding: utf-8 -*-
"""getting_started_ppo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V783IlOz3zoudmzAkYNbY94L_ZVrWz7h

# Getting started with PPO and ProcGen

Here's a bit of code that should help you get started on your projects.

The cell below installs `procgen` and downloads a small `utils.py` script that contains some utility functions. You may want to inspect the file for more details.
"""

!pip install procgen
!wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py

"""Hyperparameters. These values should be a good starting point. You can modify them later once you have a working implementation."""

# Hyperparameters
total_steps = 8e6 #1024
num_envs = 32
num_levels = 10
num_steps = 256
num_epochs = 3
batch_size = 512
eps = .2
grad_eps = .5
value_coef = .5
entropy_coef = .01
#TODO: choose appropriate values for c1 and c2
c1 = 0.5
c2 = 0.01

num_actions = 15
in_channels = 3
feature_dim = 64

make_env_kwargs = {'num_levels':200, 'env_name':'starpilot', 'use_backgrounds':False}

"""Network definitions. We have defined a policy network for you in advance. It uses the popular `NatureDQN` encoder architecture (see below), while policy and value functions are linear projections from the encodings. There is plenty of opportunity to experiment with architectures, so feel free to do that! Perhaps implement the `Impala` encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) (perhaps minus the LSTM)."""

import numpy as np
import torch
def compute_returns(rewards, discount_factor):
    """Compute discounted returns."""
    returns = np.zeros(len(rewards))
    returns[-1] = rewards[-1]
    for t in reversed(range(len(rewards)-1)):
        returns[t] = rewards[t] + discount_factor * returns[t+1]
    return returns

def clip(rt, at, eps = 0.2):
    result = []
    rtat = rt*at
    clip = torch.clip(rt,1 - eps, 1 + eps)*at
    for i in range(len(rt)):
      result.append(min(rtat[i], clip[i]))
    return torch.tensor(result).cuda()

#https://spinningup.openai.com/en/latest/algorithms/ppo.html#documentation
#https://blog.varunajayasiri.com/ml/ppo_pytorch.html
import torch
import torch.nn as nn
import torch.nn.functional as F
from utils import make_env, Storage, orthogonal_init
# import numpy as np


class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.size(0), -1)


class Encoder(nn.Module):
  def __init__(self, in_channels, feature_dim):
    super().__init__()
    self.layers = nn.Sequential(
        nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, stride=1),
        nn.MaxPool2d(3,2)
    )

    self.residual = nn.Sequential(
        nn.ReLU(),
        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding = 1),
        nn.ReLU(),
        nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding = 1),
    )

    self.residual2 = nn.Sequential(
        nn.ReLU(),
        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1),
        nn.ReLU(),
        nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1),
    )

    self.rest = nn.Sequential(
        Flatten(),
        nn.ReLU(),
        nn.Linear(in_features=28800, out_features=feature_dim),
        nn.ReLU(),
    )
    self.apply(orthogonal_init)

  def forward(self, x):
    x = self.layers(x)
    x += self.residual(x)
    x += self.residual(x)
    x = self.rest(x)

    return x


class Policy(nn.Module):
  def __init__(self, encoder, feature_dim, num_actions):
    super().__init__()
    self.encoder = encoder
    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)
    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)

  def act(self, x):
    with torch.no_grad():
      x = x.cuda().contiguous()
      dist, value = self.forward(x)
      action = dist.sample()
      log_prob = dist.log_prob(action)
    
    return action.cpu(), log_prob.cpu(), value.cpu()

  def forward(self, x):
    x = self.encoder(x)
    logits = self.policy(x)
    value = self.value(x).squeeze(1)
    dist = torch.distributions.Categorical(logits=logits)

    return dist, value


# Define environment
# check the utils.py file for info on arguments
env = make_env(**make_env_kwargs)
print('Observation space:', env.observation_space)
print('Action space:', env.action_space.n)

# Define network
encoder = Encoder(in_channels, feature_dim)
policy =  Policy(encoder, feature_dim, env.action_space.n)
policy.cuda()

# Define optimizer
# these are reasonable values but probably not optimal
optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)

# Define temporary storage
# we use this to collect transitions during each iteration
storage = Storage(
    env.observation_space.shape,
    num_steps,
    num_envs
)

# Run training
obs = env.reset()
step = 0
while step < total_steps:

  # Use policy to collect data for num_steps steps
  policy.eval()
  for _ in range(num_steps):
    # Use policy
    action, log_prob, value = policy.act(obs)
    
    # Take step in environment
    next_obs, reward, done, info = env.step(action)

    # Store data
    storage.store(obs, action, reward, done, info, log_prob, value)
    
    # Update current observation
    obs = next_obs

  # Add the last observation to collected data
  _, _, value = policy.act(obs)
  storage.store_last(obs, value)

  # Compute return and advantage
  storage.compute_return_advantage()

  # Optimize policy
  policy.train()
  for epoch in range(num_epochs):
    # Iterate over batches of transitions
    generator = storage.get_generator(batch_size)
    for batch in generator:
      b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch

      # Get current policy outputs
      new_dist, new_value = policy(b_obs)
      new_log_prob = new_dist.log_prob(b_action)

      #rewards to go
      r_t = torch.tensor(compute_returns(b_returns, 0.99)).cuda()
      #advantage estimate = discounted_rewards - value_function or baseline estimate
      #maybe b_value should be use instead of new_value
      a_t = b_advantage

      # Clipped policy objective
      #r_theta=(b_log_prob-new_log_prob).exp()
      # most probably we should do torch.mean(clip(r_thet, a_t, eps))
      # pi_loss = torch.mean(clip(r_t, a_t, eps))

      #old pi_loss = torch.mean(clip(r_t, a_t, eps))
      r_theta = (-b_log_prob+new_log_prob).exp()
      pi_loss =  torch.mean(clip(r_theta, a_t, eps))



      # # Clipped value function objective
      # b_returns maybe instead of new_value
      # value_loss = torch.mean((new_value - b_returns)**2)


      # Clipped value function objective
      clipped_value = b_value + (new_value - b_value).clamp(min=-eps, max=eps) # added
      # value_loss = (new_value - b_value)**2 # added
      value_loss = 0.5 * torch.max((b_value - b_returns) ** 2, (clipped_value - b_returns) **2).mean() # added

      # Entropy loss
      # entropy_loss = torch.mean(torch.tensor(nn.CrossEntropyLoss()).cuda())
      # entropy_loss = nn.CrossEntropyLoss()
      # ce_loss = entropy_loss(r_t, new_value)
      ce_loss = new_dist.entropy().mean()


      # Backpropagate losses
      # loss = -torch.mean(torch.mul(torch.log(b_log_prob), b_returns))
      #maybe we shouldn't use the -
      loss = -(pi_loss - c1 * value_loss + c2 * ce_loss)
      loss.backward()

      # Clip gradients
      torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)

      # Update policy
      optimizer.step()
      optimizer.zero_grad()

  # Update stats
  step += num_envs * num_steps
  print(f'Step: {step}\tMean reward: {storage.get_reward()} \t loss: {loss}')

print('Completed training!')
torch.save(policy.state_dict, 'checkpoint.pt')

"""Below cell can be used for policy evaluation and saves an episode to mp4 for you to view."""

import imageio

# Make evaluation environment
eval_env = make_env(**make_env_kwargs)
obs = eval_env.reset()

frames = []
total_reward = []

# Evaluate policy
policy.eval()
for _ in range(512):

  # Use policy
  action, log_prob, value = policy.act(obs)

  # Take step in environment
  obs, reward, done, info = eval_env.step(action)
  total_reward.append(torch.Tensor(reward))

  # Render environment and store
  frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()
  frames.append(frame)

# Calculate average return
total_reward = torch.stack(total_reward).sum(0).mean(0)
print('Average return:', total_reward)

# Save frames as video
frames = torch.stack(frames)
imageio.mimsave('vid3.mp4', frames, fps=25)