{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1_WKdcrI6w3"
      },
      "source": [
        "# Getting started with PPO and ProcGen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7LP1JU3I-d4"
      },
      "source": [
        "Here's a bit of code that should help you get started on your projects.\n",
        "\n",
        "The cell below installs `procgen` and downloads a small `utils.py` script that contains some utility functions. You may want to inspect the file for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdpZ4lmFHtD8",
        "outputId": "eb418d79-ed81-462a-f70f-097cd55aba70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting utils\n",
            "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ]
        }
      ],
      "source": [
        "#!pip install procgen\n",
        "#!wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n",
        "!pip install utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn2rkllGJPtZ"
      },
      "source": [
        "Hyperparameters. These values should be a good starting point. You can modify them later once you have a working implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8Z8P1ehENCwc"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "total_steps = 8192 #8e6\n",
        "# num_envs = 32\n",
        "# num_levels = 10\n",
        "num_steps = 256\n",
        "num_epochs = 1 #3\n",
        "batch_size = 512\n",
        "eps = .2\n",
        "grad_eps = .5\n",
        "value_coef = .5\n",
        "entropy_coef = .01\n",
        "\n",
        "num_actions = 4\n",
        "in_channels = 3\n",
        "feature_dim = 64\n",
        "\n",
        "make_env_kwargs = {'num_levels':10, 'env_name':'maze', 'use_backgrounds':True}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxRWy_T9JY4M"
      },
      "source": [
        "Network definitions. We have defined a policy network for you in advance. It uses the popular `NatureDQN` encoder architecture (see below), while policy and value functions are linear projections from the encodings. There is plenty of opportunity to experiment with architectures, so feel free to do that! Perhaps implement the `Impala` encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) (perhaps minus the LSTM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import contextlib\n",
        "import os\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "from gym import spaces\n",
        "import time\n",
        "from collections import deque\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
        "from procgen import ProcgenEnv\n",
        "from collections import deque\n",
        "\n",
        "\"\"\"\n",
        "Utility functions for the deep RL projects that I supervise in 02456 Deep Learning @ DTU.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def make_env(\n",
        "\tn_envs=32,\n",
        "\tenv_name='starpilot',\n",
        "\tstart_level=0,\n",
        "\tnum_levels=100,\n",
        "\tuse_backgrounds=False,\n",
        "\tnormalize_obs=False,\n",
        "\tnormalize_reward=True,\n",
        "\tseed=0\n",
        "\t):\n",
        "\t\"\"\"Make environment for procgen experiments\"\"\"\n",
        "\tset_global_seeds(seed)\n",
        "\tset_global_log_levels(40)\n",
        "\tenv = ProcgenEnv(\n",
        "\t\tnum_envs=n_envs,\n",
        "\t\tenv_name=env_name,\n",
        "\t\tstart_level=start_level,\n",
        "\t\tnum_levels=num_levels,\n",
        "\t\tdistribution_mode='easy',\n",
        "\t\tuse_backgrounds=use_backgrounds,\n",
        "\t\trestrict_themes=not use_backgrounds,\n",
        "\t\trender_mode='rgb_array',\n",
        "\t\trand_seed=seed\n",
        "\t)\n",
        "\tenv = VecExtractDictObs(env, \"rgb\")\n",
        "\tenv = VecNormalize(env, ob=normalize_obs, ret=normalize_reward)\n",
        "\tenv = TransposeFrame(env)\n",
        "\tenv = ScaledFloatFrame(env)\n",
        "\tenv = TensorEnv(env)\n",
        "\t\n",
        "\treturn env\n",
        "\n",
        "\n",
        "class Storage():\n",
        "\tdef __init__(self, obs_shape, num_steps, num_envs, gamma=0.99, lmbda=0.95, normalize_advantage=True):\n",
        "\t\tself.obs_shape = obs_shape\n",
        "\t\tself.num_steps = num_steps\n",
        "\t\tself.num_envs = num_envs\n",
        "\t\tself.gamma = gamma\n",
        "\t\tself.lmbda = lmbda\n",
        "\t\tself.normalize_advantage = normalize_advantage\n",
        "\t\tself.reset()\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tself.obs = torch.zeros(self.num_steps+1, self.num_envs, *self.obs_shape)\n",
        "\t\tself.action = torch.zeros(self.num_steps, self.num_envs)\n",
        "\t\tself.reward = torch.zeros(self.num_steps, self.num_envs)\n",
        "\t\tself.done = torch.zeros(self.num_steps, self.num_envs)\n",
        "\t\tself.log_prob = torch.zeros(self.num_steps, self.num_envs)\n",
        "\t\tself.value = torch.zeros(self.num_steps+1, self.num_envs)\n",
        "\t\tself.returns = torch.zeros(self.num_steps, self.num_envs)\n",
        "\t\tself.advantage = torch.zeros(self.num_steps, self.num_envs)\n",
        "\t\tself.info = deque(maxlen=self.num_steps)\n",
        "\t\tself.step = 0\n",
        "\n",
        "\tdef store(self, obs, action, reward, done, info, log_prob, value):\n",
        "\t\tself.obs[self.step] = obs.clone()\n",
        "\t\tself.action[self.step] = action.clone()\n",
        "\t\tself.reward[self.step] = torch.from_numpy(reward.copy())\n",
        "\t\tself.done[self.step] = torch.from_numpy(done.copy())\n",
        "\t\tself.info.append(info)\n",
        "\t\tself.log_prob[self.step] = log_prob.clone()\n",
        "\t\tself.value[self.step] = value.clone()\n",
        "\t\tself.step = (self.step + 1) % self.num_steps\n",
        "\n",
        "\tdef store_last(self, obs, value):\n",
        "\t\tself.obs[-1] = obs.clone()\n",
        "\t\tself.value[-1] = value.clone()\n",
        "\n",
        "\tdef compute_return_advantage(self):\n",
        "\t\tadvantage = 0\n",
        "\t\tfor i in reversed(range(self.num_steps)):\n",
        "\t\t\tdelta = (self.reward[i] + self.gamma * self.value[i+1] * (1 - self.done[i])) - self.value[i]\n",
        "\t\t\tadvantage = self.gamma * self.lmbda * advantage * (1 - self.done[i]) + delta\n",
        "\t\t\tself.advantage[i] = advantage\n",
        "\n",
        "\t\tself.returns = self.advantage + self.value[:-1]\n",
        "\t\tif self.normalize_advantage:\n",
        "\t\t\tself.advantage = (self.advantage - self.advantage.mean()) / (self.advantage.std() + 1e-9)\n",
        "\n",
        "\tdef get_generator(self, batch_size=1024):\n",
        "\t\titerator = BatchSampler(SubsetRandomSampler(range(self.num_steps*self.num_envs)), batch_size, drop_last=True)\n",
        "\t\tfor indices in iterator:\n",
        "\t\t\tobs = self.obs[:-1].reshape(-1, *self.obs_shape)[indices].cuda()\n",
        "\t\t\taction = self.action.reshape(-1)[indices].cuda()\n",
        "\t\t\tlog_prob = self.log_prob.reshape(-1)[indices].cuda()\n",
        "\t\t\tvalue = self.value[:-1].reshape(-1)[indices].cuda()\n",
        "\t\t\treturns = self.returns.reshape(-1)[indices].cuda()\n",
        "\t\t\tadvantage = self.advantage.reshape(-1)[indices].cuda()\n",
        "\t\t\tyield obs, action, log_prob, value, returns, advantage\n",
        "\n",
        "\tdef get_reward(self, normalized_reward=True):\n",
        "\t\tif normalized_reward:\n",
        "\t\t\treward = []\n",
        "\t\t\tfor step in range(self.num_steps):\n",
        "\t\t\t\tinfo = self.info[step]\n",
        "\t\t\t\treward.append([d['reward'] for d in info])\n",
        "\t\t\treward = torch.Tensor(reward)\n",
        "\t\telse:\n",
        "\t\t\treward = self.reward\n",
        "\t\t\n",
        "\t\treturn reward.mean(1).sum(0)\n",
        "\n",
        "\n",
        "def orthogonal_init(module, gain=nn.init.calculate_gain('relu')):\n",
        "\t\"\"\"Orthogonal weight initialization: https://arxiv.org/abs/1312.6120\"\"\"\n",
        "\tif isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
        "\t\tnn.init.orthogonal_(module.weight.data, gain)\n",
        "\t\tnn.init.constant_(module.bias.data, 0)\n",
        "\treturn module\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Helper functions that set global seeds and gym logging preferences\n",
        "\"\"\"\n",
        "\n",
        "def set_global_seeds(seed):\n",
        "\ttorch.backends.cudnn.deterministic = True\n",
        "\ttorch.backends.cudnn.benchmark = False\n",
        "\ttorch.manual_seed(seed)\n",
        "\ttorch.cuda.manual_seed_all(seed)\n",
        "\tnp.random.seed(seed)\n",
        "\trandom.seed(seed)\n",
        "\n",
        "\n",
        "def set_global_log_levels(level):\n",
        "\tgym.logger.set_level(level)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Copy-pasted from OpenAI to obviate dependency on Baselines. Required for vectorized environments.\n",
        "You will never have to look beyond this line.\n",
        "\"\"\"\n",
        "\n",
        "class AlreadySteppingError(Exception):\n",
        "\t\"\"\"\n",
        "\tRaised when an asynchronous step is running while\n",
        "\tstep_async() is called again.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tmsg = 'already running an async step'\n",
        "\t\tException.__init__(self, msg)\n",
        "\n",
        "\n",
        "class NotSteppingError(Exception):\n",
        "\t\"\"\"\n",
        "\tRaised when an asynchronous step is not running but\n",
        "\tstep_wait() is called.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tmsg = 'not running an async step'\n",
        "\t\tException.__init__(self, msg)\n",
        "\n",
        "\n",
        "class VecEnv(ABC):\n",
        "\t\"\"\"\n",
        "\tAn abstract asynchronous, vectorized environment.\n",
        "\tUsed to batch data from multiple copies of an environment, so that\n",
        "\teach observation becomes an batch of observations, and expected action is a batch of actions to\n",
        "\tbe applied per-environment.\n",
        "\t\"\"\"\n",
        "\tclosed = False\n",
        "\tviewer = None\n",
        "\n",
        "\tmetadata = {\n",
        "\t\t'render.modes': ['human', 'rgb_array']\n",
        "\t}\n",
        "\n",
        "\tdef __init__(self, num_envs, observation_space, action_space):\n",
        "\t\tself.num_envs = num_envs\n",
        "\t\tself.observation_space = observation_space\n",
        "\t\tself.action_space = action_space\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef reset(self):\n",
        "\t\t\"\"\"\n",
        "\t\tReset all the environments and return an array of\n",
        "\t\tobservations, or a dict of observation arrays.\n",
        "\n",
        "\t\tIf step_async is still doing work, that work will\n",
        "\t\tbe cancelled and step_wait() should not be called\n",
        "\t\tuntil step_async() is invoked again.\n",
        "\t\t\"\"\"\n",
        "\t\tpass\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef step_async(self, actions):\n",
        "\t\t\"\"\"\n",
        "\t\tTell all the environments to start taking a step\n",
        "\t\twith the given actions.\n",
        "\t\tCall step_wait() to get the results of the step.\n",
        "\n",
        "\t\tYou should not call this if a step_async run is\n",
        "\t\talready pending.\n",
        "\t\t\"\"\"\n",
        "\t\tpass\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef step_wait(self):\n",
        "\t\t\"\"\"\n",
        "\t\tWait for the step taken with step_async().\n",
        "\n",
        "\t\tReturns (obs, rews, dones, infos):\n",
        "\t\t - obs: an array of observations, or a dict of\n",
        "\t\t\t\tarrays of observations.\n",
        "\t\t - rews: an array of rewards\n",
        "\t\t - dones: an array of \"episode done\" booleans\n",
        "\t\t - infos: a sequence of info objects\n",
        "\t\t\"\"\"\n",
        "\t\tpass\n",
        "\n",
        "\tdef close_extras(self):\n",
        "\t\t\"\"\"\n",
        "\t\tClean up the  extra resources, beyond what's in this base class.\n",
        "\t\tOnly runs when not self.closed.\n",
        "\t\t\"\"\"\n",
        "\t\tpass\n",
        "\n",
        "\tdef close(self):\n",
        "\t\tif self.closed:\n",
        "\t\t\treturn\n",
        "\t\tif self.viewer is not None:\n",
        "\t\t\tself.viewer.close()\n",
        "\t\tself.close_extras()\n",
        "\t\tself.closed = True\n",
        "\n",
        "\tdef step(self, actions):\n",
        "\t\t\"\"\"\n",
        "\t\tStep the environments synchronously.\n",
        "\n",
        "\t\tThis is available for backwards compatibility.\n",
        "\t\t\"\"\"\n",
        "\t\tself.step_async(actions)\n",
        "\t\treturn self.step_wait()\n",
        "\n",
        "\tdef render(self, mode='human'):\n",
        "\t\timgs = self.get_images()\n",
        "\t\tbigimg = \"ARGHH\" #tile_images(imgs)\n",
        "\t\tif mode == 'human':\n",
        "\t\t\tself.get_viewer().imshow(bigimg)\n",
        "\t\t\treturn self.get_viewer().isopen\n",
        "\t\telif mode == 'rgb_array':\n",
        "\t\t\treturn bigimg\n",
        "\t\telse:\n",
        "\t\t\traise NotImplementedError\n",
        "\n",
        "\tdef get_images(self):\n",
        "\t\t\"\"\"\n",
        "\t\tReturn RGB images from each environment\n",
        "\t\t\"\"\"\n",
        "\t\traise NotImplementedError\n",
        "\n",
        "\t@property\n",
        "\tdef unwrapped(self):\n",
        "\t\tif isinstance(self, VecEnvWrapper):\n",
        "\t\t\treturn self.venv.unwrapped\n",
        "\t\telse:\n",
        "\t\t\treturn self\n",
        "\n",
        "\tdef get_viewer(self):\n",
        "\t\tif self.viewer is None:\n",
        "\t\t\tfrom gym.envs.classic_control import rendering\n",
        "\t\t\tself.viewer = rendering.SimpleImageViewer()\n",
        "\t\treturn self.viewer\n",
        "\n",
        "\t\n",
        "class VecEnvWrapper(VecEnv):\n",
        "\t\"\"\"\n",
        "\tAn environment wrapper that applies to an entire batch\n",
        "\tof environments at once.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, venv, observation_space=None, action_space=None):\n",
        "\t\tself.venv = venv\n",
        "\t\tsuper().__init__(num_envs=venv.num_envs,\n",
        "\t\t\t\t\t\tobservation_space=observation_space or venv.observation_space,\n",
        "\t\t\t\t\t\taction_space=action_space or venv.action_space)\n",
        "\n",
        "\tdef step_async(self, actions):\n",
        "\t\tself.venv.step_async(actions)\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef reset(self):\n",
        "\t\tpass\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef step_wait(self):\n",
        "\t\tpass\n",
        "\n",
        "\tdef close(self):\n",
        "\t\treturn self.venv.close()\n",
        "\n",
        "\tdef render(self, mode='human'):\n",
        "\t\treturn self.venv.render(mode=mode)\n",
        "\n",
        "\tdef get_images(self):\n",
        "\t\treturn self.venv.get_images()\n",
        "\n",
        "\tdef __getattr__(self, name):\n",
        "\t\tif name.startswith('_'):\n",
        "\t\t\traise AttributeError(\"attempted to get missing private attribute '{}'\".format(name))\n",
        "\t\treturn getattr(self.venv, name)\n",
        "\n",
        "\t\n",
        "class VecEnvObservationWrapper(VecEnvWrapper):\n",
        "\t@abstractmethod\n",
        "\tdef process(self, obs):\n",
        "\t\tpass\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tobs = self.venv.reset()\n",
        "\t\treturn self.process(obs)\n",
        "\n",
        "\tdef step_wait(self):\n",
        "\t\tobs, rews, dones, infos = self.venv.step_wait()\n",
        "\t\treturn self.process(obs), rews, dones, infos\n",
        "\n",
        "\t\n",
        "class CloudpickleWrapper(object):\n",
        "\t\"\"\"\n",
        "\tUses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, x):\n",
        "\t\tself.x = x\n",
        "\n",
        "\tdef __getstate__(self):\n",
        "\t\timport cloudpickle\n",
        "\t\treturn cloudpickle.dumps(self.x)\n",
        "\n",
        "\tdef __setstate__(self, ob):\n",
        "\t\timport pickle\n",
        "\t\tself.x = pickle.loads(ob)\n",
        "\n",
        "\t\t\n",
        "@contextlib.contextmanager\n",
        "def clear_mpi_env_vars():\n",
        "\t\"\"\"\n",
        "\tfrom mpi4py import MPI will call MPI_Init by default.  If the child process has MPI environment variables, MPI will think that the child process is an MPI process just like the parent and do bad things such as hang.\n",
        "\tThis context manager is a hacky way to clear those environment variables temporarily such as when we are starting multiprocessing\n",
        "\tProcesses.\n",
        "\t\"\"\"\n",
        "\tremoved_environment = {}\n",
        "\tfor k, v in list(os.environ.items()):\n",
        "\t\tfor prefix in ['OMPI_', 'PMI_']:\n",
        "\t\t\tif k.startswith(prefix):\n",
        "\t\t\t\tremoved_environment[k] = v\n",
        "\t\t\t\tdel os.environ[k]\n",
        "\ttry:\n",
        "\t\tyield\n",
        "\tfinally:\n",
        "\t\tos.environ.update(removed_environment)\n",
        "\n",
        "\t\t\n",
        "class VecFrameStack(VecEnvWrapper):\n",
        "\tdef __init__(self, venv, nstack):\n",
        "\t\tself.venv = venv\n",
        "\t\tself.nstack = nstack\n",
        "\t\twos = venv.observation_space  # wrapped ob space\n",
        "\t\tlow = np.repeat(wos.low, self.nstack, axis=-1)\n",
        "\t\thigh = np.repeat(wos.high, self.nstack, axis=-1)\n",
        "\t\tself.stackedobs = np.zeros((venv.num_envs,) + low.shape, low.dtype)\n",
        "\t\tobservation_space = spaces.Box(low=low, high=high, dtype=venv.observation_space.dtype)\n",
        "\t\tVecEnvWrapper.__init__(self, venv, observation_space=observation_space)\n",
        "\n",
        "\tdef step_wait(self):\n",
        "\t\tobs, rews, news, infos = self.venv.step_wait()\n",
        "\t\tself.stackedobs = np.roll(self.stackedobs, shift=-1, axis=-1)\n",
        "\t\tfor (i, new) in enumerate(news):\n",
        "\t\t\tif new:\n",
        "\t\t\t\tself.stackedobs[i] = 0\n",
        "\t\tself.stackedobs[..., -obs.shape[-1]:] = obs\n",
        "\t\treturn self.stackedobs, rews, news, infos\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tobs = self.venv.reset()\n",
        "\t\tself.stackedobs[...] = 0\n",
        "\t\tself.stackedobs[..., -obs.shape[-1]:] = obs\n",
        "\t\treturn self.stackedobs\n",
        "\t\n",
        "class VecExtractDictObs(VecEnvObservationWrapper):\n",
        "\tdef __init__(self, venv, key):\n",
        "\t\tself.key = key\n",
        "\t\tsuper().__init__(venv=venv,\n",
        "\t\t\tobservation_space=venv.observation_space.spaces[self.key])\n",
        "\n",
        "\tdef process(self, obs):\n",
        "\t\treturn obs[self.key]\n",
        "\t\n",
        "\t\n",
        "class RunningMeanStd(object):\n",
        "\t# https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
        "\tdef __init__(self, epsilon=1e-4, shape=()):\n",
        "\t\tself.mean = np.zeros(shape, 'float64')\n",
        "\t\tself.var = np.ones(shape, 'float64')\n",
        "\t\tself.count = epsilon\n",
        "\n",
        "\tdef update(self, x):\n",
        "\t\tbatch_mean = np.mean(x, axis=0)\n",
        "\t\tbatch_var = np.var(x, axis=0)\n",
        "\t\tbatch_count = x.shape[0]\n",
        "\t\tself.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "\tdef update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "\t\tself.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
        "\t\t\tself.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
        "\n",
        "\t\t\n",
        "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
        "\tdelta = batch_mean - mean\n",
        "\ttot_count = count + batch_count\n",
        "\n",
        "\tnew_mean = mean + delta * batch_count / tot_count\n",
        "\tm_a = var * count\n",
        "\tm_b = batch_var * batch_count\n",
        "\tM2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
        "\tnew_var = M2 / tot_count\n",
        "\tnew_count = tot_count\n",
        "\n",
        "\treturn new_mean, new_var, new_count\n",
        "\n",
        "\n",
        "class VecNormalize(VecEnvWrapper):\n",
        "\t\"\"\"\n",
        "\tA vectorized wrapper that normalizes the observations\n",
        "\tand returns from an environment.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, venv, ob=True, ret=True, clipob=10., cliprew=10., gamma=0.99, epsilon=1e-8):\n",
        "\t\tVecEnvWrapper.__init__(self, venv)\n",
        "\n",
        "\t\tself.ob_rms = RunningMeanStd(shape=self.observation_space.shape) if ob else None\n",
        "\t\tself.ret_rms = RunningMeanStd(shape=()) if ret else None\n",
        "\t\t\n",
        "\t\tself.clipob = clipob\n",
        "\t\tself.cliprew = cliprew\n",
        "\t\tself.ret = np.zeros(self.num_envs)\n",
        "\t\tself.gamma = gamma\n",
        "\t\tself.epsilon = epsilon\n",
        "\n",
        "\tdef step_wait(self):\n",
        "\t\tobs, rews, news, infos = self.venv.step_wait()\n",
        "\t\tfor i in range(len(infos)):\n",
        "\t\t\tinfos[i]['reward'] = rews[i]\n",
        "\t\tself.ret = self.ret * self.gamma + rews\n",
        "\t\tobs = self._obfilt(obs)\n",
        "\t\tif self.ret_rms:\n",
        "\t\t\tself.ret_rms.update(self.ret)\n",
        "\t\t\trews = np.clip(rews / np.sqrt(self.ret_rms.var + self.epsilon), -self.cliprew, self.cliprew)\n",
        "\t\tself.ret[news] = 0.\n",
        "\t\treturn obs, rews, news, infos\n",
        "\n",
        "\tdef _obfilt(self, obs):\n",
        "\t\tif self.ob_rms:\n",
        "\t\t\tself.ob_rms.update(obs)\n",
        "\t\t\tobs = np.clip((obs - self.ob_rms.mean) / np.sqrt(self.ob_rms.var + self.epsilon), -self.clipob, self.clipob)\n",
        "\t\t\treturn obs\n",
        "\t\telse:\n",
        "\t\t\treturn obs\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tself.ret = np.zeros(self.num_envs)\n",
        "\t\tobs = self.venv.reset()\n",
        "\t\treturn self._obfilt(obs)\n",
        "\n",
        "\n",
        "class TransposeFrame(VecEnvWrapper):\n",
        "\tdef __init__(self, env):\n",
        "\t\tsuper().__init__(venv=env)\n",
        "\t\tobs_shape = self.observation_space.shape\n",
        "\t\tself.observation_space = gym.spaces.Box(low=0, high=255, shape=(obs_shape[2], obs_shape[0], obs_shape[1]), dtype=np.float32)\n",
        "\n",
        "\tdef step_wait(self):\n",
        "\t\tobs, reward, done, info = self.venv.step_wait()\n",
        "\t\treturn obs.transpose(0,3,1,2), reward, done, info\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tobs = self.venv.reset()\n",
        "\t\treturn obs.transpose(0,3,1,2)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(VecEnvWrapper):\n",
        "\tdef __init__(self, env):\n",
        "\t\tsuper().__init__(venv=env)\n",
        "\t\tobs_shape = self.observation_space.shape\n",
        "\t\tself.observation_space = gym.spaces.Box(low=0, high=1, shape=obs_shape, dtype=np.float32)\n",
        "\n",
        "\tdef step_wait(self):\n",
        "\t\tobs, reward, done, info = self.venv.step_wait()\n",
        "\t\treturn obs/255.0, reward, done, info\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tobs = self.venv.reset()\n",
        "\t\treturn obs/255.0\n",
        "\n",
        "\n",
        "class TensorEnv(VecEnvWrapper):\n",
        "\tdef __init__(self, env):\n",
        "\t\tsuper().__init__(venv=env)\n",
        "\n",
        "\tdef step_async(self, actions):\n",
        "\t\tif isinstance(actions, torch.Tensor):\n",
        "\t\t\tactions = actions.detach().cpu().numpy()\n",
        "\t\tself.venv.step_async(actions)\n",
        "\n",
        "\tdef step_wait(self):\n",
        "\t\tobs, reward, done, info = self.venv.step_wait()\n",
        "\t\treturn torch.Tensor(obs), reward, done, info\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tobs = self.venv.reset()\n",
        "\t\treturn torch.Tensor(obs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "yTBV9xpKpEFa",
        "outputId": "4d0f4f52-5091-4e68-a29c-be3db783ac97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation space: Box([[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]], [[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]]], (3, 64, 64), float32)\n",
            "Action space: 15\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19216/1772034225.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# Use policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;31m# Take step in environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19216/1772034225.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m       \u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m       \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Johan\\projects\\Deep Learning Project\\env\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             raise AssertionError(\n",
            "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, in_channels, feature_dim):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),\n",
        "        Flatten(),\n",
        "        nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()\n",
        "    )\n",
        "    self.apply(orthogonal_init)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "  def __init__(self, encoder, feature_dim, num_actions):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n",
        "    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n",
        "\n",
        "  def act(self, x):\n",
        "    with torch.no_grad():\n",
        "      x = x.cuda().contiguous()\n",
        "      dist, value = self.forward(x)\n",
        "      action = dist.sample()\n",
        "      log_prob = dist.log_prob(action)\n",
        "    \n",
        "    return action.cpu(), log_prob.cpu(), value.cpu()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    logits = self.policy(x)\n",
        "    value = self.value(x).squeeze(1)\n",
        "    dist = torch.distributions.Categorical(logits=logits)\n",
        "\n",
        "    return dist, value\n",
        "\n",
        "\n",
        "# Define environment\n",
        "# check the utils.py file for info on arguments\n",
        "env = make_env(**make_env_kwargs)\n",
        "print('Observation space:', env.observation_space)\n",
        "print('Action space:', env.action_space.n)\n",
        "\n",
        "# Define network\n",
        "encoder = Encoder(in_channels, feature_dim)\n",
        "policy =  Policy(encoder, feature_dim, env.action_space.n)\n",
        "policy.cuda()\n",
        "\n",
        "# Define optimizer\n",
        "# these are reasonable values but probably not optimal\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\n",
        "\n",
        "# Define temporary storage\n",
        "# we use this to collect transitions during each iteration\n",
        "storage = Storage(\n",
        "    env.observation_space.shape,\n",
        "    num_steps,\n",
        "    num_envs\n",
        ")\n",
        "\n",
        "# Run training\n",
        "obs = env.reset()\n",
        "step = 0\n",
        "while step < total_steps:\n",
        "\n",
        "  # Use policy to collect data for num_steps steps\n",
        "  policy.eval()\n",
        "  for _ in range(num_steps):\n",
        "    # Use policy\n",
        "    action, log_prob, value = policy.act(obs)\n",
        "    \n",
        "    # Take step in environment\n",
        "    next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "    # Store data\n",
        "    storage.store(obs, action, reward, done, info, log_prob, value)\n",
        "    \n",
        "    # Update current observation\n",
        "    obs = next_obs\n",
        "\n",
        "  # Add the last observation to collected data\n",
        "  _, _, value = policy.act(obs)\n",
        "  storage.store_last(obs, value)\n",
        "\n",
        "  # Compute return and advantage\n",
        "  storage.compute_return_advantage()\n",
        "\n",
        "  # Optimize policy\n",
        "  policy.train()\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    # Iterate over batches of transitions\n",
        "    generator = storage.get_generator(batch_size)\n",
        "    for batch in generator:\n",
        "      b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n",
        "\n",
        "      # Get current policy outputs\n",
        "      new_dist, new_value = policy(b_obs)\n",
        "      new_log_prob = new_dist.log_prob(b_action)\n",
        "\n",
        "      # Clipped policy objective\n",
        "      # pi_loss = \n",
        "\n",
        "      # Clipped value function objective\n",
        "      # value_loss = \n",
        "\n",
        "      # Entropy loss\n",
        "      # entropy_loss = \n",
        "\n",
        "      # Backpropagate losses\n",
        "      # loss = \n",
        "      # loss.backward()\n",
        "\n",
        "      # Clip gradients\n",
        "      torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n",
        "\n",
        "      # Update policy\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "  # Update stats\n",
        "  step += num_envs * num_steps\n",
        "  print(f'Step: {step}\\tMean reward: {storage.get_reward()}')\n",
        "\n",
        "print('Completed training!')\n",
        "torch.save(policy.state_dict, 'checkpoint.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAZrWuVGLTu-"
      },
      "source": [
        "Below cell can be used for policy evaluation and saves an episode to mp4 for you to view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zecOCkd7Jzt"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "\n",
        "# Make evaluation environment\n",
        "eval_env = make_env(**make_env_kwargs)\n",
        "obs = eval_env.reset()\n",
        "\n",
        "frames = []\n",
        "total_reward = []\n",
        "\n",
        "# Evaluate policy\n",
        "policy.eval()\n",
        "for _ in range(512):\n",
        "\n",
        "  # Use policy\n",
        "  action, log_prob, value = policy.act(obs)\n",
        "\n",
        "  # Take step in environment\n",
        "  obs, reward, done, info = eval_env.step(action)\n",
        "  total_reward.append(torch.Tensor(reward))\n",
        "\n",
        "  # Render environment and store\n",
        "  frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\n",
        "  frames.append(frame)\n",
        "\n",
        "# Calculate average return\n",
        "total_reward = torch.stack(total_reward).sum(0).mean(0)\n",
        "print('Average return:', total_reward)\n",
        "\n",
        "# Save frames as video\n",
        "frames = torch.stack(frames)\n",
        "imageio.mimsave('vid.mp4', frames, fps=25)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "getting-started-ppo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
